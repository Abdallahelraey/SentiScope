{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\AI\\\\NLP\\\\HandsOn\\\\sentiment-analysis'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Union, Optional\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class FeatureTransformConfig:\n",
    "    root_dir: Path\n",
    "    data_file_path: Path\n",
    "    data_file: Path\n",
    "    features_dir: Path\n",
    "    text_column: str\n",
    "    sentiment_column: str\n",
    "    train_size: float\n",
    "    random_state: int\n",
    "    vectorizer_type: str  # 'tfidf', 'bow', or 'word2vec'\n",
    "    max_features: int\n",
    "    ngram_range: Tuple[int, int]\n",
    "    word2vec_params: Optional[Dict] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SentiScope.constants import (CONFIG_FILE_PATH,\n",
    "                                  PARAMS_FILE_PATH)\n",
    "from SentiScope.utils.file_utils import (create_directories,\n",
    "                                            get_size)\n",
    "from SentiScope.utils.config_utils import (read_yaml,\n",
    "                                           Settings,\n",
    "                                           get_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "    def get_latest_report(self) -> Dict:\n",
    "        \"\"\"Locate the latest report.json file based on the timestamp folder.\"\"\"\n",
    "        config = self.config.data_profileing\n",
    "        profiling_dir = Path(config.root_dir)\n",
    "\n",
    "        # Get all subdirectories in data_profiling\n",
    "        timestamp_dirs = [d for d in profiling_dir.iterdir() if d.is_dir()]\n",
    "        \n",
    "        if not timestamp_dirs:\n",
    "            raise FileNotFoundError(\"No timestamp folders found in data_profiling.\")\n",
    "\n",
    "        # Sort directories by name (assuming timestamp format)\n",
    "        latest_dir = sorted(timestamp_dirs, key=lambda x: x.name, reverse=True)[0]\n",
    "        report_path = latest_dir / \"report.json\"\n",
    "\n",
    "        if not report_path.exists():\n",
    "            raise FileNotFoundError(f\"report.json not found in {latest_dir}.\")\n",
    "\n",
    "        # Load the report.json file\n",
    "        with open(report_path, \"r\") as f:\n",
    "            report_data = json.load(f)\n",
    "\n",
    "        return report_data\n",
    "\n",
    "    def get_feature_transform_config(self) -> FeatureTransformConfig:\n",
    "        config = self.config.feature_transformation\n",
    "        report_data = self.get_latest_report()\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        timestamp = report_data[\"timestamp\"]\n",
    "        # data_file_path = Path(str(config.data_file).format(timestamp=timestamp))\n",
    "        data_file_path = Path(config.data_file_path).joinpath(f\"{timestamp}\", config.data_file)\n",
    " \n",
    "\n",
    "        feature_transform_config = FeatureTransformConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_file=config.data_file,\n",
    "            data_file_path = data_file_path,\n",
    "            features_dir=config.features_dir,\n",
    "            text_column=config.text_column,\n",
    "            sentiment_column=config.sentiment_column,\n",
    "            train_size=config.train_size,\n",
    "            random_state=config.random_state,\n",
    "            vectorizer_type=config.vectorizer_type,\n",
    "            max_features=config.max_features,\n",
    "            ngram_range=tuple(config.ngram_range),\n",
    "            word2vec_params=config.word2vec_params if hasattr(config, 'word2vec_params') else None\n",
    "        )\n",
    "\n",
    "        return feature_transform_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "from SentiScope.logging import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransformer:\n",
    "    def __init__(self, config: FeatureTransformConfig):\n",
    "        \"\"\"\n",
    "        Initialize the FeatureTransformer with configuration settings.\n",
    "        \n",
    "        Parameters:\n",
    "        config (FeatureTransformConfig): Configuration object containing transformation parameters\n",
    "        \"\"\"\n",
    "        logger.info(\"Initializing FeatureTransformer...\")\n",
    "        self.config = config\n",
    "        self.path = self.config.data_file_path\n",
    "        self.df = pd.read_csv(self.path)\n",
    "        \n",
    "        # Create output directories\n",
    "        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        self.output_dir = Path(self.config.root_dir) / self.timestamp\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Initialize encoders and vectorizers\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.vectorizer = self.config.vectorizer_type\n",
    "        self.word2vec_model = None\n",
    "        \n",
    "        logger.info(\"FeatureTransformer initialized successfully.\")\n",
    "\n",
    "    def _split_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Split the data into training and testing sets.\n",
    "        \"\"\"\n",
    "        logger.info(\"Splitting data into train and test sets...\")\n",
    "        \n",
    "        # Check class distribution if stratification is requested\n",
    "        if self.config.sentiment_column:\n",
    "            class_counts = self.df[self.config.sentiment_column].value_counts()\n",
    "            min_samples = class_counts.min()\n",
    "            \n",
    "            if min_samples < 2:\n",
    "                logger.warning(f\"Found class(es) with less than 2 samples. Disabling stratification.\")\n",
    "                stratify = None\n",
    "            else:\n",
    "                stratify = self.df[self.config.sentiment_column]\n",
    "        else:\n",
    "            stratify = None\n",
    "        \n",
    "        train_df, test_df = train_test_split(\n",
    "            self.df,\n",
    "            train_size=self.config.train_size,\n",
    "            random_state=self.config.random_state,\n",
    "            stratify=stratify\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Train set size: {len(train_df)}, Test set size: {len(test_df)}\")\n",
    "        return train_df, test_df\n",
    "\n",
    "    def _initialize_vectorizer(self):\n",
    "        \"\"\"\n",
    "        Initialize the appropriate vectorizer based on configuration.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Initializing {self.config.vectorizer_type} vectorizer...\")\n",
    "        if self.config.vectorizer_type == 'tfidf':\n",
    "            self.vectorizer = TfidfVectorizer(\n",
    "                max_features=self.config.max_features,\n",
    "                ngram_range=self.config.ngram_range\n",
    "            )\n",
    "        elif self.config.vectorizer_type == 'bow':\n",
    "            self.vectorizer = CountVectorizer(\n",
    "                max_features=self.config.max_features,\n",
    "                ngram_range=self.config.ngram_range\n",
    "            )\n",
    "        elif self.config.vectorizer_type == 'word2vec':\n",
    "            # Word2Vec will be initialized during transformation\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported vectorizer type: {self.config.vectorizer_type}\")\n",
    "\n",
    "    def _transform_text_features(self, train_df: pd.DataFrame, test_df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Transform text data into numerical features using the specified method.\n",
    "        \"\"\"\n",
    "        logger.info(\"Transforming text features...\")\n",
    "        \n",
    "        if self.config.vectorizer_type in ['tfidf', 'bow']:\n",
    "            # Transform using TF-IDF or Bag-of-Words\n",
    "            X_train = self.vectorizer.fit_transform(train_df[self.config.text_column])\n",
    "            X_test = self.vectorizer.transform(test_df[self.config.text_column])\n",
    "            \n",
    "            # Save vectorizer\n",
    "            joblib.dump(self.vectorizer, self.output_dir / f'{self.config.vectorizer_type}_vectorizer.joblib')\n",
    "            \n",
    "        elif self.config.vectorizer_type == 'word2vec':\n",
    "            # Initialize and train Word2Vec model\n",
    "            texts = train_df[self.config.text_column].apply(str.split).values\n",
    "            self.word2vec_model = Word2Vec(\n",
    "                sentences=texts,\n",
    "                vector_size=self.config.word2vec_params.get('vector_size', 100),\n",
    "                window=self.config.word2vec_params.get('window', 5),\n",
    "                min_count=self.config.word2vec_params.get('min_count', 1),\n",
    "                workers=self.config.word2vec_params.get('workers', 4)\n",
    "            )\n",
    "            \n",
    "            # Transform texts to vectors by averaging word vectors\n",
    "            X_train = np.array([\n",
    "                np.mean([self.word2vec_model.wv[word] \n",
    "                        for word in text.split() \n",
    "                        if word in self.word2vec_model.wv], axis=0)\n",
    "                for text in train_df[self.config.text_column]\n",
    "            ])\n",
    "            X_test = np.array([\n",
    "                np.mean([self.word2vec_model.wv[word]\n",
    "                        for word in text.split()\n",
    "                        if word in self.word2vec_model.wv], axis=0)\n",
    "                for text in test_df[self.config.text_column]\n",
    "            ])\n",
    "            \n",
    "            # Save Word2Vec model\n",
    "            self.word2vec_model.save(str(self.output_dir / 'word2vec_model.model'))\n",
    "            \n",
    "        logger.info(\"Text feature transformation completed.\")\n",
    "        return X_train, X_test\n",
    "\n",
    "    def _transform_labels(self, train_df: pd.DataFrame, test_df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Transform labels using LabelEncoder.\n",
    "        \"\"\"\n",
    "        if not self.config.sentiment_column:\n",
    "            logger.info(\"No sentiment column specified. Skipping label transformation.\")\n",
    "            return None, None\n",
    "            \n",
    "        logger.info(\"Transforming labels...\")\n",
    "        y_train = self.label_encoder.fit_transform(train_df[self.config.sentiment_column])\n",
    "        y_test = self.label_encoder.transform(test_df[self.config.sentiment_column])\n",
    "        \n",
    "        # Save label encoder\n",
    "        joblib.dump(self.label_encoder, self.output_dir / 'label_encoder.joblib')\n",
    "        \n",
    "        return y_train, y_test\n",
    "\n",
    "    def transform_and_save(self) -> str:\n",
    "        \"\"\"\n",
    "        Execute the complete transformation pipeline and save results.\n",
    "        \n",
    "        Returns:\n",
    "        str: Path to the output directory\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Starting feature transformation pipeline...\")\n",
    "            \n",
    "            # Split data\n",
    "            train_df, test_df = self._split_data()\n",
    "            \n",
    "            # Initialize vectorizer\n",
    "            self._initialize_vectorizer()\n",
    "            \n",
    "            # Transform features\n",
    "            X_train, X_test = self._transform_text_features(train_df, test_df)\n",
    "            \n",
    "            # Transform labels\n",
    "            y_train, y_test = self._transform_labels(train_df, test_df)\n",
    "            \n",
    "            # Save transformed data\n",
    "            np.save(self.output_dir / 'X_train.npy', X_train)\n",
    "            np.save(self.output_dir / 'X_test.npy', X_test)\n",
    "            if y_train is not None and y_test is not None:\n",
    "                np.save(self.output_dir / 'y_train.npy', y_train)\n",
    "                np.save(self.output_dir / 'y_test.npy', y_test)\n",
    "            \n",
    "            # Save configuration and metadata\n",
    "            metadata = {\n",
    "                'timestamp': self.timestamp,\n",
    "                'config': {\n",
    "                    'vectorizer_type': self.config.vectorizer_type,\n",
    "                    'max_features': self.config.max_features,\n",
    "                    'ngram_range': self.config.ngram_range,\n",
    "                    'train_size': self.config.train_size,\n",
    "                    'random_state': self.config.random_state\n",
    "                },\n",
    "                'data_shapes': {\n",
    "                    'X_train': X_train.shape,\n",
    "                    'X_test': X_test.shape,\n",
    "                    'y_train': y_train.shape if y_train is not None else None,\n",
    "                    'y_test': y_test.shape if y_test is not None else None\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            with open(self.output_dir / 'metadata.json', 'w') as f:\n",
    "                json.dump(metadata, f, indent=4)\n",
    "            \n",
    "            logger.info(f\"Feature transformation completed. Results saved to: {self.output_dir}\")\n",
    "            return str(self.output_dir)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during feature transformation: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-09 18:48:23,156: INFO: config_utils: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-01-09 18:48:23,158: INFO: config_utils: yaml file: params.yaml loaded successfully]\n",
      "[2025-01-09 18:48:23,159: INFO: file_utils: created directory at: artifacts]\n",
      "[2025-01-09 18:48:23,161: INFO: file_utils: created directory at: artifacts/feature_transformation]\n",
      "[2025-01-09 18:48:23,162: INFO: 3257807759: Initializing FeatureTransformer...]\n",
      "[2025-01-09 18:48:24,125: INFO: 3257807759: FeatureTransformer initialized successfully.]\n",
      "[2025-01-09 18:48:24,126: INFO: 3257807759: Starting feature transformation pipeline...]\n",
      "[2025-01-09 18:48:24,126: INFO: 3257807759: Splitting data into train and test sets...]\n",
      "[2025-01-09 18:48:24,316: INFO: 3257807759: Train set size: 175435, Test set size: 43859]\n",
      "[2025-01-09 18:48:24,317: INFO: 3257807759: Initializing bow vectorizer...]\n",
      "[2025-01-09 18:48:24,317: INFO: 3257807759: Transforming text features...]\n",
      "[2025-01-09 18:48:35,695: INFO: 3257807759: Text feature transformation completed.]\n",
      "[2025-01-09 18:48:35,696: INFO: 3257807759: Transforming labels...]\n",
      "[2025-01-09 18:48:35,948: INFO: 3257807759: Feature transformation completed. Results saved to: artifacts\\feature_transformation\\20250109_184824]\n",
      "[2025-01-09 18:48:35,966: INFO: 2519662654: Feature transformation completed. Output saved at: artifacts\\feature_transformation\\20250109_184824]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    feature_transform_config = config.get_feature_transform_config()\n",
    "    transformer = FeatureTransformer(config=feature_transform_config)\n",
    "    output_path = transformer.transform_and_save()\n",
    "    logger.info(f\"Feature transformation completed. Output saved at: {output_path}\")\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SentiScope-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
