{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\AI\\\\NLP\\\\HandsOn\\\\sentiment-analysis'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataProfilerConfig:\n",
    "    root_dir: Path\n",
    "    data_file: Path\n",
    "    profile_folder: Path\n",
    "    profile_file: Path\n",
    "    text_column: str\n",
    "    sentiment_column: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SentiScope.constants import (CONFIG_FILE_PATH,\n",
    "                                  PARAMS_FILE_PATH)\n",
    "from SentiScope.utils.file_utils import (create_directories,\n",
    "                                            get_size)\n",
    "from SentiScope.utils.config_utils import (read_yaml,\n",
    "                                           Settings,\n",
    "                                           get_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "import json\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    def get_latest_report_profiler(self) -> Dict:\n",
    "        \"\"\"Locate the latest report.json file based on the timestamp folder.\"\"\"\n",
    "        config = self.config.data_ingestion\n",
    "        profiling_dir = Path(config.root_dir)\n",
    "\n",
    "        # Get all subdirectories in data_profiling\n",
    "        timestamp_dirs = [d for d in profiling_dir.iterdir() if d.is_dir()]\n",
    "        \n",
    "        if not timestamp_dirs:\n",
    "            raise FileNotFoundError(\"No timestamp folders found in data_profiling.\")\n",
    "\n",
    "        # Sort directories by name (assuming timestamp format)\n",
    "        latest_dir = sorted(timestamp_dirs, key=lambda x: x.name, reverse=True)[0]\n",
    "        report_path = latest_dir / \"ingestion_metadata.json\"\n",
    "\n",
    "        if not report_path.exists():\n",
    "            raise FileNotFoundError(f\"report.json not found in {latest_dir}.\")\n",
    "\n",
    "        # Load the report.json file\n",
    "        with open(report_path, \"r\") as f:\n",
    "            report_data = json.load(f)\n",
    "\n",
    "        return report_data\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def get_data_profiler_config(self) -> DataProfilerConfig:\n",
    "        config = self.config.data_profileing\n",
    "        report_data = self.get_latest_report_profiler()\n",
    "        \n",
    "        timestamp = report_data[\"timestamp\"]\n",
    "        # data_file_path = Path(str(config.data_file).format(timestamp=timestamp))\n",
    "        data_file_path = Path(config.data_file).joinpath(f\"{timestamp}\",\"unzipped\", \"file.csv\")\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_profileing_config = DataProfilerConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_file=data_file_path,\n",
    "            profile_folder= config.profile_folder,\n",
    "            profile_file= config.profile_file,\n",
    "            text_column = config.text_column,\n",
    "            sentiment_column = config.sentiment_column\n",
    "        )\n",
    "\n",
    "        return data_profileing_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "from typing import Dict, List, Optional, Any\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import emoji\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from SentiScope.logging import logger\n",
    "\n",
    "class SentimentDataProfiler:\n",
    "    def __init__(self, config: DataProfilerConfig):\n",
    "        \"\"\"\n",
    "        Initialize the SentimentDataProfiler with a data path and column names.\n",
    "        \n",
    "        Parameters:\n",
    "        data_path (str): Path to the CSV file containing sentiment data\n",
    "        text_column (str): Name of the column containing text data\n",
    "        sentiment_column (str, optional): Name of the column containing sentiment labels\n",
    "        \"\"\"\n",
    "        # Convert string path to Path object\n",
    "        logger.info(\"Initializing SentimentDataProfiler...\")\n",
    "        self.config = config\n",
    "        self.path = self.config.data_file\n",
    "        logger.info(f\"Reading CSV file from path: {self.path}\")\n",
    "        self.df = self._read_csv_file(self.path)\n",
    "        self.text_column = self.config.text_column\n",
    "        self.sentiment_column = self.config.sentiment_column\n",
    "        \n",
    "        # Initialize NLTK components\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "            nltk.data.find('corpora/stopwords')\n",
    "            logger.info(\"Required NLTK data found.\")\n",
    "        except LookupError:\n",
    "            logger.info(\"Downloading required NLTK data...\")\n",
    "            nltk.download('punkt')\n",
    "            nltk.download('stopwords')\n",
    "            nltk.download('wordnet')\n",
    "        \n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        \n",
    "        # Create output directory structure\n",
    "        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        self.output_dir = Path(os.getcwd()) / self.config.profile_folder / self.timestamp\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        (self.output_dir / 'images').mkdir(exist_ok=True)\n",
    "        logger.info(f\"Output directory created at {self.output_dir}\")\n",
    "        \n",
    "        # Validate and process\n",
    "        self._validate_columns()\n",
    "        logger.info(\"Columns validated successfully.\")\n",
    "        self.df['processed_text'] = self.df[self.text_column].apply(self._preprocess_text)\n",
    "        logger.info(\"Text preprocessing completed.\")\n",
    "\n",
    "    def _read_csv_file(self, file_path: Path) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Read and validate the CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        file_path (Path): Path to the CSV file\n",
    "        \n",
    "        Returns:\n",
    "        pd.DataFrame: The loaded DataFrame\n",
    "        \"\"\"\n",
    "        logger.info(f\"Reading CSV file from {file_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            logger.info(\"Successfully read the CSV file.\")\n",
    "            return df\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"Error: File not found at {file_path}\")\n",
    "            raise\n",
    "        except pd.errors.ParserError:\n",
    "            logger.error(\"Error: There might be a parsing issue with the CSV file!\")\n",
    "            try:\n",
    "                # Attempt to read with more flexible parsing\n",
    "                df = pd.read_csv(file_path, dtype=str)\n",
    "                return df\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to fix parsing errors: {e}\")\n",
    "                raise\n",
    "\n",
    "    def _validate_columns(self) -> None:\n",
    "        \"\"\"Validate that the specified columns exist in the DataFrame.\"\"\"\n",
    "        logger.info(\"Validating columns in the DataFrame.\")\n",
    "        if self.text_column not in self.df.columns:\n",
    "            logger.error(f\"Text column '{self.text_column}' not found in DataFrame.\")\n",
    "            raise ValueError(f\"Text column '{self.text_column}' not found in DataFrame\")\n",
    "        if self.sentiment_column and self.sentiment_column not in self.df.columns:\n",
    "            logger.error(f\"Sentiment column '{self.sentiment_column}' not found in DataFrame.\")\n",
    "            raise ValueError(f\"Sentiment column '{self.sentiment_column}' not found in DataFrame\")\n",
    "\n",
    "    def _preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Preprocess a single text string by removing links, emojis, converting to lowercase,\n",
    "        tokenizing, removing stop words, stemming, and concatenating into a single string.\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            logger.warning(\"Non-string text encountered during preprocessing.\")\n",
    "            return \"\"\n",
    "        \n",
    "        logger.debug(f\"Preprocessing text: {text[:30]}...\")\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove emojis using emoji library\n",
    "        text = emoji.replace_emoji(text, '')\n",
    "        \n",
    "        text = text.lower()\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [self.stemmer.stem(token) for token in tokens\n",
    "                if token.isalnum() and token not in self.stop_words]\n",
    "        \n",
    "        # Concatenate tokens into a single string with spaces between them\n",
    "        processed_text = \" \".join(tokens)\n",
    "        \n",
    "        logger.debug(f\"Processed text: {processed_text[:50]}\")\n",
    "        return processed_text\n",
    "\n",
    "    def _get_initial_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate basic statistics about the dataset.\"\"\"\n",
    "        logger.info(\"Generating initial dataset statistics.\")\n",
    "        stats = {\n",
    "            'total_rows': len(self.df),\n",
    "            'total_columns': len(self.df.columns),\n",
    "            'dtypes': {k: str(v) for k, v in self.df.dtypes.to_dict().items()},  # Convert dtypes to strings for JSON\n",
    "            'missing_values': self.df.isnull().sum().to_dict()\n",
    "        }\n",
    "\n",
    "        self.df['text_length'] = self.df[self.text_column].str.len()\n",
    "        stats['text_length_stats'] = {\n",
    "            'mean': int(self.df['text_length'].mean()),\n",
    "            'median': int(self.df['text_length'].median()),\n",
    "            'min': int(self.df['text_length'].min()),\n",
    "            'max': int(self.df['text_length'].max())\n",
    "        }\n",
    "        logger.info(\"Initial statistics generated successfully.\")\n",
    "        return stats\n",
    "\n",
    "    def _analyze_text_features(self) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze text features and generate visualizations.\"\"\"\n",
    "        # Get all words from processed texts\n",
    "        logger.info(\"Analyzing text features.\")\n",
    "        all_words = [word for text in self.df['processed_text'] for word in text]\n",
    "        word_freq = Counter(all_words)\n",
    "        logger.info(\"Text feature analysis completed.\")\n",
    "        vocab_stats = {\n",
    "            'total_words': len(all_words),\n",
    "            'unique_words': len(word_freq),\n",
    "            'average_words_per_text': round(len(all_words) / len(self.df), 2),\n",
    "            'most_common_words': dict(word_freq.most_common(20))\n",
    "        }\n",
    "        \n",
    "        # Generate and save word cloud\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(all_words))\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title('Word Cloud of Text Data')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / 'images' / 'wordcloud.png')\n",
    "        plt.close()\n",
    "        logger.info(f\"Word Cloud saved to: {self.output_dir}/'images'/'wordcloud.png'\")\n",
    "        return vocab_stats\n",
    "\n",
    "    def _analyze_sentiment_distribution(self) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Analyze sentiment distribution and generate visualization.\"\"\"\n",
    "        \n",
    "        if not self.sentiment_column:\n",
    "            logger.info(\"No sentiment column provided; skipping sentiment analysis.\")\n",
    "            return None\n",
    "        logger.info(\"Analyzing sentiment distribution.\")\n",
    "        sentiment_stats = {\n",
    "            'value_counts': self.df[self.sentiment_column].value_counts().to_dict(),\n",
    "            'distribution_percentage': {k: round(v, 2) for k, v in \n",
    "                (self.df[self.sentiment_column].value_counts(normalize=True) * 100).to_dict().items()}\n",
    "        }\n",
    "        \n",
    "        # Generate and save sentiment distribution plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.countplot(data=self.df, x=self.df[self.sentiment_column])\n",
    "        plt.title('Sentiment Distribution')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / 'images' / 'sentiment_distribution.png')\n",
    "        plt.close()\n",
    "        logger.info(\"Sentiment distribution analysis completed.\")\n",
    "        logger.info(f\"Sentiment distribution plot saved to: {self.output_dir} / 'images' / 'sentiment_distribution.png' \")\n",
    "        return sentiment_stats\n",
    "\n",
    "    def save_dataframe(self, filename: str = \"processed_data.csv\") -> str:\n",
    "        \"\"\"\n",
    "        Save the processed DataFrame to the output directory as a CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filename : str, optional\n",
    "            The name of the output file. Default is \"processed_data.csv\".\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            Path to the saved CSV file.\n",
    "        \"\"\"\n",
    "        file_path = self.output_dir / filename\n",
    "        try:\n",
    "            self.df.to_csv(file_path, index=False)\n",
    "            logger.info(f\"DataFrame successfully saved to {file_path}\")\n",
    "            return str(file_path)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save DataFrame: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "\n",
    "    def generate_report(self) -> str:\n",
    "        \"\"\"\n",
    "        Generate and save a comprehensive profile report.\n",
    "        \n",
    "        Returns:\n",
    "        str: Path to the generated report directory\n",
    "        \"\"\"\n",
    "        logger.info(\"Generating profile report.\")\n",
    "        # Generate report components\n",
    "        report = {\n",
    "            'timestamp': self.timestamp,\n",
    "            'dataset_info': {\n",
    "                'text_column': self.text_column,\n",
    "                'sentiment_column': self.sentiment_column\n",
    "            },\n",
    "            'initial_statistics': self._get_initial_statistics(),\n",
    "            'text_analysis': self._analyze_text_features()\n",
    "        }\n",
    "        \n",
    "        if self.sentiment_column:\n",
    "            report['sentiment_analysis'] = self._analyze_sentiment_distribution()\n",
    "            \n",
    "        # Save Processed data\n",
    "        self.save_dataframe()\n",
    "        \n",
    "        # Save report as JSON\n",
    "        report_path = self.output_dir / 'report.json'\n",
    "        with open(report_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(report, f, indent=4, ensure_ascii=False)\n",
    "        logger.info(f\"Report generated successfully at {report_path}.\")\n",
    "        # Generate a README with file descriptions\n",
    "        readme_content = f\"\"\"Data Profile Report\n",
    "        Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "        Files in this directory:\n",
    "        1. report.json - Complete analysis report in JSON format\n",
    "        2. images/wordcloud.png - Word cloud visualization of text data\"\"\"\n",
    "\n",
    "        if self.sentiment_column:\n",
    "            readme_content += \"\\n3. images/sentiment_distribution.png - Distribution of sentiment labels\"\n",
    "\n",
    "        with open(self.output_dir / 'README.txt', 'w') as f:\n",
    "            f.write(readme_content)\n",
    "        logger.info(\"README file created successfully.\")\n",
    "        return str(self.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-16 04:28:04,118: INFO: config_utils: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-01-16 04:28:04,120: INFO: config_utils: yaml file: params.yaml loaded successfully]\n",
      "[2025-01-16 04:28:04,121: INFO: file_utils: created directory at: artifacts]\n",
      "[2025-01-16 04:28:04,122: INFO: file_utils: created directory at: artifacts/data_profileing]\n",
      "[2025-01-16 04:28:04,122: INFO: 1607497536: Initializing SentimentDataProfiler...]\n",
      "[2025-01-16 04:28:04,123: INFO: 1607497536: Reading CSV file from path: artifacts\\data_ingestion\\20250114_001312\\unzipped\\file.csv]\n",
      "[2025-01-16 04:28:04,124: INFO: 1607497536: Reading CSV file from artifacts\\data_ingestion\\20250114_001312\\unzipped\\file.csv]\n",
      "[2025-01-16 04:28:04,773: INFO: 1607497536: Successfully read the CSV file.]\n",
      "[2025-01-16 04:28:04,774: INFO: 1607497536: Required NLTK data found.]\n",
      "[2025-01-16 04:28:04,776: INFO: 1607497536: Output directory created at d:\\AI\\NLP\\HandsOn\\sentiment-analysis\\artifacts\\data_profileing\\20250116_042804]\n",
      "[2025-01-16 04:28:04,777: INFO: 1607497536: Validating columns in the DataFrame.]\n",
      "[2025-01-16 04:28:04,778: INFO: 1607497536: Columns validated successfully.]\n",
      "[2025-01-16 04:29:42,627: INFO: 1607497536: Text preprocessing completed.]\n",
      "[2025-01-16 04:29:42,634: INFO: 1607497536: Generating profile report.]\n",
      "[2025-01-16 04:29:42,634: INFO: 1607497536: Generating initial dataset statistics.]\n",
      "[2025-01-16 04:29:42,731: INFO: 1607497536: Initial statistics generated successfully.]\n",
      "[2025-01-16 04:29:42,732: INFO: 1607497536: Analyzing text features.]\n",
      "[2025-01-16 04:29:43,835: INFO: 1607497536: Text feature analysis completed.]\n",
      "[2025-01-16 04:30:05,556: INFO: 1607497536: Word Cloud saved to: d:\\AI\\NLP\\HandsOn\\sentiment-analysis\\artifacts\\data_profileing\\20250116_042804/'images'/'wordcloud.png']\n",
      "[2025-01-16 04:30:05,580: INFO: 1607497536: Analyzing sentiment distribution.]\n",
      "[2025-01-16 04:30:06,006: INFO: 1607497536: Sentiment distribution analysis completed.]\n",
      "[2025-01-16 04:30:06,006: INFO: 1607497536: Sentiment distribution plot saved to: d:\\AI\\NLP\\HandsOn\\sentiment-analysis\\artifacts\\data_profileing\\20250116_042804 / 'images' / 'sentiment_distribution.png' ]\n",
      "[2025-01-16 04:30:07,992: INFO: 1607497536: DataFrame successfully saved to d:\\AI\\NLP\\HandsOn\\sentiment-analysis\\artifacts\\data_profileing\\20250116_042804\\processed_data.csv]\n",
      "[2025-01-16 04:30:07,993: INFO: 1607497536: Report generated successfully at d:\\AI\\NLP\\HandsOn\\sentiment-analysis\\artifacts\\data_profileing\\20250116_042804\\report.json.]\n",
      "[2025-01-16 04:30:07,994: INFO: 1607497536: README file created successfully.]\n",
      "[2025-01-16 04:30:07,995: INFO: 2799083353: Report generated at: d:\\AI\\NLP\\HandsOn\\sentiment-analysis\\artifacts\\data_profileing\\20250116_042804]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_profiler_config = config.get_data_profiler_config()\n",
    "    profiler = SentimentDataProfiler(config=data_profiler_config)\n",
    "    report_path = profiler.generate_report()\n",
    "    logger.info(f\"Report generated at: {report_path}\")\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SentiScope-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
